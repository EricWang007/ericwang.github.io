<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML on EricWang's Blog</title><link>https://ericwang007.github.io/posts/ml/</link><description>Recent content in ML on EricWang's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 05 Aug 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://ericwang007.github.io/posts/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>AndrewNG-CV基础</title><link>https://ericwang007.github.io/posts/ml/andrewng-cv%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-cv%E5%9F%BA%E7%A1%80/</guid><description>AndrewNG-CV 基础 1 The Basics of Convolutional Neural Networks 1.1 Edge detection Use filter to do the convolution operation
One Example Convolution function in tensorflow: tf.nn.conv2d
Other Examples 1-&amp;gt;-1: light-&amp;gt;dark -1-&amp;gt;1: dark-&amp;gt;light Furthermore, treat the 9 numbers as parameters, and use backward propagation to improve them.
1.2 Padding(填充) To preserve the information on the edges and corners.
Valid convolutions: No padding $n\times n$ * $f\times f$​ ——&amp;gt; $(n-f+1)\times(n-f+1)$ Same convolutions: Pad so that output size is the same as the input size.</description></item><item><title>AndrewNG-DL基础</title><link>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%9F%BA%E7%A1%80/</guid><description>AndrewNG-Deep Learning 基础 1 Logistic Regression Model 1.1 Binary Classification To learn a classifier that can input an image represented by the feature vector x, and predict the corresponding label y.
Notation——n training examples: ($ n_x $为向量维数，$X$为$ n_x\times m $矩阵) $$ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), &amp;hellip;, (x^{(m)},y^{(m)}),x\in R^{n_x}, y\in {0,1} $$
$$ X=[x^{(1)},x^{(2)},&amp;hellip;,x^{(m)}], X\in R^{n\times m} $$
$$ Y=[y^{(1)},y^{(2)},&amp;hellip;,y^{(m)}], Y\in R^{1\times m} $$
1.2 Logistic Regression An algorithm for binary classification problems.</description></item><item><title>AndrewNG-DL应用</title><link>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%BA%94%E7%94%A8/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%BA%94%E7%94%A8/</guid><description>AndrewNG-DL 应用 1 Setting up ML application 1.1 Train/dev/test set Training set Validation/Development set: used for selecting model. Test set: used for assessment of the generalization error of the final chosen model. In previous era, we with limited data, we use 60/20/20 for tain/dev/test.
In Big data era, we use 99% of data as training set.
Make sure dev and test set come from same distribution.</description></item><item><title>AndrewNG-GAN基础</title><link>https://ericwang007.github.io/posts/ml/andrewng-gan%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-gan%E5%9F%BA%E7%A1%80/</guid><description>AndrewNG-GAN Course 1 —— Build Basic GANs 1.1 Introduction Generative Models: Variational Autoencoders(VAE):
GANS:
GAN in Real Life GAN的创始人：Ian Goodfellow GAN的应用领域： Image Generation, Deep fake Text Generation Data Augmentaion Image Filters 1.2 Basic Components Discriminator Use Neural Networks, input: features(image), output: probability
0.85这个概率也会交给Generator
Input features e.g.: RGB pixel values for images
Generator Use Neural Networks, input: class+noise vector, output: features(image)</description></item><item><title>李宏毅-机器学习2021春-1</title><link>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-1/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-1/</guid><description>李宏毅-机器学习2021春-1 1 机器学习基本概念 1.1 机器学习基本任务 机器学习的基本任务：寻找一个函数
不同种类的函数：
Regression（回归）：函数输出一个标量 如：预测PM2.5 Clssificatiion（分类）：给定选项，函数输出选项 如：Alpha Go下棋 Structured Learning：创造一些结构（图片，文件） 1.2 通过训练数据定义Loss Loss 也是一个函数，它的输入是Model中的parameters： $$ L(b,w) $$
Loss function：$L=\frac{1}{N}\sum_ne_n$
Mean Absolute Error(MAE)：$e=|y-\hat{y}|$ Mean Square Error(MSE)：$e=(y-\hat{y})^2$ 1.3 Optimization 目标：得到最优的参数。 $$ w^{}, b^{}=\arg \min _{w, b} L $$ 方式：Gradient Descent
一个参数w的情况 随机选取初始值$w^0$ 计算$\left.\frac{\partial L}{\partial \mathcal{\imath}}\right|_{w=w^{0}}$ learning rate：$\eta$，表示梯度下降的速率 不断更新w：$w^{1} \leftarrow w^{0}-\left.\eta \frac{\partial L}{\partial w}\right|_{w=w^{0}}$ 两个参数的情况： $$ \frac{\partial L}{\partial w}|{w=w^{0}, b=b^{0}} \quad w^{1} \leftarrow w^{0}-\eta \frac{\partial L}{\partial w}|{w=w^{0}, b=b^{0}} \ \frac{\partial L}{\partial b}|{w=w^{0}, b=b^{0}} \quad b^{1} \leftarrow b^{0}- \eta \frac{\partial L}{\partial b}|{w=w^{0}, b=b^{0}} $$</description></item></channel></rss>