<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML on EricWang's Blog</title><link>https://ericwang007.github.io/posts/ml/</link><description>Recent content in ML on EricWang's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 17 Oct 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://ericwang007.github.io/posts/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>李宏毅-机器学习2021春-2</title><link>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-2/</link><pubDate>Sun, 17 Oct 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-2/</guid><description>李宏毅-机器学习2021春-2 1 机器学习任务攻略 Model bias：模型过于简单，有局限性
Optimization Issue：模型足够复杂了，但Optimization做的不够好。
Overfitting：在训练集上效果好，在测试集上效果差。解决方法如下：
增加训练数据，Data augmentation（图片左右翻转） 限制模型，减少模型的flexibility： 减少参数，共享参数（CNN） 减少features Early stopping Regularization Dropout 模型的复杂性需要在Overfitting和Model bias间进行trade-off
mismatch：训练集和测试集的数据分别不同
N-fold Cross Validation：将Training Set进行不同的分组，分别对模型进行训练和验证，得到的mse取平均值。
2 类神经网络训练不起来怎么办 2.1 Local Minima与Saddle Point Critical point包括Local Minima与Saddle Point。
当有很多参数时，Local Minima几乎不存在。
2.2 Batch与Momentum Batch Shuffle：在一个epoch重新分batch。 大Batch v.s. 小Batch
当考虑并行运算（gpu）后，大Batch的运行速度更快。
小Batch的noise更大，但这反而有助于训练。</description></item><item><title>李宏毅-机器学习2021春-3</title><link>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-3/</link><pubDate>Sun, 17 Oct 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-3/</guid><description>李宏毅-机器学习2021春-3 1 Classification 将Class用one-hot vector表示
回归与分类的区别：
softmax：
Loss的计算：
2 Convolutional Neural Network (CNN) 3 Self-attention 3.1 背景 当输入是多组向量时，输出的情况：
每一个向量都有一个label（sequence labeling），用到了self-attention 整个sequence有一个label 模型自己决定输出长度。 3.2 原理 Self-attention：根据a向量之间的关联性，相应的b向量
解释得到$b^1$的过程：
●代表dot-product；此处不一定要用soft-max 完整过程：
3.3 变种 在语音辨识中，用于输入的数据很长，可以用Trancated Self-attention，只考虑每一段音频和周围的一定长度内的音频的关系。</description></item><item><title>李宏毅-机器学习2021春-4</title><link>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-4/</link><pubDate>Sun, 17 Oct 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-4/</guid><description>李宏毅-机器学习2021春-4 1 Transformer Sequence-to-sequence（Seq2seq）
输出的长度由模型决定 Encoder+Decoder 1.1 Encoder 1.2 Decoder-Autoregressive(AT)</description></item><item><title>李宏毅-机器学习2021春-1</title><link>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-1/</link><pubDate>Fri, 15 Oct 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-1/</guid><description>李宏毅-机器学习2021春-1 1 机器学习基本概念 1.1 机器学习基本任务 机器学习的基本任务：寻找一个函数
不同种类的函数：
Regression（回归）：函数输出一个标量 如：预测PM2.5 Clssificatiion（分类）：给定选项，函数输出选项 如：Alpha Go下棋 Structured Learning：创造一些结构（图片，文件） 1.2 通过训练数据定义Loss Loss 也是一个函数，它的输入是Model中的parameters： $$ L(b,w) $$
Loss function：$L=\frac{1}{N}\sum_ne_n$
Mean Absolute Error(MAE)：$e=|y-\hat{y}|$ Mean Square Error(MSE)：$e=(y-\hat{y})^2$ 1.3 Optimization 目标：得到最优的参数。 $$ w^, b^=arg min _{w, b} L $$ 方式：Gradient Descent
一个参数w的情况 随机选取初始值$w^0$ 计算$\left.\frac{\partial L}{\partial \mathcal{\imath}}\right|_{w=w^{0}}$ learning rate：$\eta$，表示梯度下降的速率 不断更新w：$w^{1} \leftarrow w^{0}-\left.\eta \frac{\partial L}{\partial w}\right|_{w=w^{0}}$ 两个参数的情况： 2 深度学习基本概念 2.</description></item><item><title>AndrewNG-CV基础</title><link>https://ericwang007.github.io/posts/ml/andrewng-cv%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-cv%E5%9F%BA%E7%A1%80/</guid><description>AndrewNG-CV 基础 1 The Basics of Convolutional Neural Networks 1.1 Edge detection Use filter to do the convolution operation
One Example Convolution function in tensorflow: tf.nn.conv2d
Other Examples 1-&amp;gt;-1: light-&amp;gt;dark -1-&amp;gt;1: dark-&amp;gt;light Furthermore, treat the 9 numbers as parameters, and use backward propagation to improve them.
1.2 Padding(填充) To preserve the information on the edges and corners.
Valid convolutions: No padding $n\times n$ * $f\times f$​ ——&amp;gt; $(n-f+1)\times(n-f+1)$ Same convolutions: Pad so that output size is the same as the input size.</description></item><item><title>AndrewNG-DL基础</title><link>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%9F%BA%E7%A1%80/</guid><description>AndrewNG-Deep Learning 基础 1 Logistic Regression Model 1.1 Binary Classification To learn a classifier that can input an image represented by the feature vector x, and predict the corresponding label y.
Notation——n training examples: ($ n_x $为向量维数，$X$为$ n_x\times m $矩阵) $$ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), &amp;hellip;, (x^{(m)},y^{(m)}),x\in R^{n_x}, y\in {0,1} $$
$$ X=[x^{(1)},x^{(2)},&amp;hellip;,x^{(m)}], X\in R^{n\times m} $$
$$ Y=[y^{(1)},y^{(2)},&amp;hellip;,y^{(m)}], Y\in R^{1\times m} $$
1.2 Logistic Regression An algorithm for binary classification problems.</description></item><item><title>AndrewNG-DL应用</title><link>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%BA%94%E7%94%A8/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-dl%E5%BA%94%E7%94%A8/</guid><description>AndrewNG-DL 应用 1 Setting up ML application 1.1 Train/dev/test set Training set Validation/Development set: used for selecting model. Test set: used for assessment of the generalization error of the final chosen model. In previous era, we with limited data, we use 60/20/20 for tain/dev/test.
In Big data era, we use 99% of data as training set.
Make sure dev and test set come from same distribution.</description></item><item><title>AndrewNG-GAN基础</title><link>https://ericwang007.github.io/posts/ml/andrewng-gan%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 05 Aug 2021 06:00:20 +0600</pubDate><guid>https://ericwang007.github.io/posts/ml/andrewng-gan%E5%9F%BA%E7%A1%80/</guid><description>AndrewNG-GAN Course 1 —— Build Basic GANs 1.1 Introduction Generative Models: Variational Autoencoders(VAE):
GANS:
GAN in Real Life GAN的创始人：Ian Goodfellow GAN的应用领域： Image Generation, Deep fake Text Generation Data Augmentaion Image Filters 1.2 Basic Components Discriminator Use Neural Networks, input: features(image), output: probability
0.85这个概率也会交给Generator
Input features e.g.: RGB pixel values for images
Generator Use Neural Networks, input: class+noise vector, output: features(image)</description></item></channel></rss>