<!doctype html><html><head><title>DL基础</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><meta property="og:title" content="DL基础"><meta property="og:description" content="Deep Learning 基础  1 Logistic Regression Model 1.1 Binary Classification  To learn a classifier that can input an image represented by the feature vector x, and predict the corresponding label y.
 Notation——n training examples: ($ n_x $为向量维数，$X$为$ n_x\times m $矩阵) $$ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), &mldr;, (x^{(m)},y^{(m)}),x\in R^{n_x}, y\in {0,1} $$
$$ X=[x^{(1)},x^{(2)},&mldr;,x^{(m)}], X\in R^{n\times m} $$
$$ Y=[y^{(1)},y^{(2)},&mldr;,y^{(m)}], Y\in R^{1\times m} $$
1.2 Logistic Regression  An algorithm for binary classification problems."><meta property="og:type" content="article"><meta property="og:url" content="https://ericwang007.github.io/posts/ml/dl%E5%9F%BA%E7%A1%80/"><meta property="article:published_time" content="2021-08-05T06:00:20+06:00"><meta property="article:modified_time" content="2021-08-05T06:00:20+06:00"><meta name=description content="DL基础"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>EricWang's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithmdatastructure/>Algorithm&DataStructure</a><ul><li><a href=/posts/algorithmdatastructure/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/ title=排序方法总结>排序方法总结</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/java/>Java</a><ul><li><a href=/posts/java/java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/ title=Java基础知识>Java基础知识</a></li><li><a href=/posts/java/java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/ title=Java集合框架>Java集合框架</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/ml/>ML</a><ul class=active><li><a href=/posts/ml/cv%E5%9F%BA%E7%A1%80/ title=CV基础>CV基础</a></li><li><a class=active href=/posts/ml/dl%E5%9F%BA%E7%A1%80/ title=DL基础>DL基础</a></li><li><a href=/posts/ml/dl%E5%BA%94%E7%94%A8/ title=DL应用>DL应用</a></li><li><a href=/posts/ml/gan%E5%9F%BA%E7%A1%80/ title=GAN基础>GAN基础</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://ericwang007.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/%E5%86%88%E5%B4%8E%E6%9C%8B%E4%B9%9F_huadb4a3dbe9f5476167756f348ab52644_35855_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Eric Wang</h5><p>August 5, 2021</p></div><div class=title><h1>DL基础</h1></div><div class=post-content id=post-content><h1 id=deep-learning-基础>Deep Learning 基础</h1><hr><h2 id=1-logistic-regression-model>1 Logistic Regression Model</h2><h3 id=11-binary-classification>1.1 Binary Classification</h3><blockquote><p>To learn a classifier that can input an image represented by the feature vector <code>x</code>, and predict the corresponding label <code>y</code>.</p></blockquote><p><strong>Notation——n training examples</strong>: ($ n_x $为向量维数，$X$为$ n_x\times m $矩阵)
$$
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), &mldr;, (x^{(m)},y^{(m)}),x\in R^{n_x}, y\in {0,1}
$$</p><p>$$
X=[x^{(1)},x^{(2)},&mldr;,x^{(m)}], X\in R^{n\times m}
$$</p><p>$$
Y=[y^{(1)},y^{(2)},&mldr;,y^{(m)}], Y\in R^{1\times m}
$$</p><h3 id=12-logistic-regression>1.2 Logistic Regression</h3><blockquote><p>An algorithm for <em>binary classification</em> problems.</p></blockquote><p>Given <code>x</code>, we want $\hat{y}=P(y=1|x)$​</p><p><strong>Way</strong>:</p><ul><li><p>Parameters: $w\in R^{nx},b\in R$</p></li><li><p>Output: （$\sigma$为sigmoid函数，$\sigma(z)=\frac{1}{1+e^{-z}}$）</p><p>$$
\hat{y}=\sigma(w^Tx+b), (z = w^Tx+b)
$$</p></li></ul><h3 id=13-cost-function>1.3 Cost Function</h3><p>we want $\hat{y(i)}\approx y(i)$</p><p><strong>Loss(error) function</strong>: $L(\hat{y},y)=-(y·log\hat{y}+(1-y)log(1-\hat{y}))$​</p><ul><li>$\hat{y(i)}$与$y(i)$越接近，$L(\hat{y},y)$越小</li><li>If $y=1$: $L(\hat{y},y)=-log\hat{y}$</li><li>If $y=0$: $L(\hat{y},y)=-log(1-\hat{y})$</li></ul><blockquote><p>It measures how well you&rsquo;re doing <u>on a single training example</u>.</p></blockquote><p><strong>Cost function</strong>:$J(w,b)=\frac{1}{m}\sum_{i = 1}^{m}L(\hat{y}^{(i)},y^{(i)})$</p><blockquote><p>It measures how well you&rsquo;re doing <u>on the entire training set</u>.</p></blockquote><h3 id=14-gradient-descent>1.4 Gradient Descent</h3><blockquote><p>An algorithm to learn the parameteres <code>w</code> and <code>b</code> on your training set.</p></blockquote><p>$$
w := w-\alpha\frac{\partial J(w,b)}{\partial w}, b:=b-\alpha\frac{\partial J(w,b)}{\partial b}
$$</p><p>$\alpha$​​​​ is the <u>learning rate</u>.</p><p>Derivatives (导数)</p><h4 id=computation-graph>Computation Graph</h4><p><u>backward propagation</u></p><p><img src=/images/posts/ML/image-20210806212057599.png alt=image-20210806212057599></p><p>在代码中，可以用<code>dv</code>，<code>da</code>作为变量名。</p><p>$$
\frac{dJ}{dc}=\frac{dJ}{dv}\frac{dv}{du}\frac{du}{dc}=3\times1\times3=9
$$</p><h4 id=gradient-descent-on-1-example>Gradient Descent on <em>1</em> example</h4><p><img src=/images/posts/ML/image-20210806213958503.png alt=image-20210806213958503>
$$
w_1 := w_1-\alpha\frac{\partial dL}{\partial w},w_2 := w_2-\alpha\frac{\partial dL}{\partial w2},b := b-\alpha\frac{\partial dL}{\partial b}
$$</p><h4 id=gradient-descent-on-m-examples>Gradient Descent on <em>m</em> examples</h4><blockquote><p>Neural network programming guideline: Whenever possible, avoid explicit for-loops.</p></blockquote><blockquote><p>Use <u>vectorization</u>.</p></blockquote><p><strong>Forward Propagation:</strong>
$$
A=\sigma\left(w^{T} X+b\right)=\left(a^{(1)}, a^{(2)}, \ldots, a^{(m-1)}, a^{(m)}\right)
$$</p><ul><li><strong>Cost function:</strong></li></ul><p>$$
J=-\frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)} \log \left(a^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-a^{(i)}\right)\right)
$$</p><p><strong>Backward Propagation</strong>:</p><p>$$
\frac{\partial J}{\partial w}=\frac{1}{m} X(A-Y)^{T}
$$</p><p>$$
\frac{\partial J}{\partial b}=\frac{1}{m} \sum_{i=1}^{m}\left(a^{(i)}-y^{(i)}\right)
$$</p><p><strong>Code of Optimizing procedure:</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>propagate</span>(w, b, X, Y):
    m <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
    
    <span style=color:#75715e># Forward propogation</span>
    A <span style=color:#f92672>=</span> sigmoid(np<span style=color:#f92672>.</span>dot(w<span style=color:#f92672>.</span>T, X)<span style=color:#f92672>+</span>b);
    cost <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>average(Y<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log(A)<span style=color:#f92672>+</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>Y)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>A))
    
    <span style=color:#75715e>#Backward propagation</span>
    dw <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X, (A<span style=color:#f92672>-</span>Y)<span style=color:#f92672>.</span>T)<span style=color:#f92672>/</span>m
    db <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>average(A<span style=color:#f92672>-</span>Y)

    cost <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>squeeze(np<span style=color:#f92672>.</span>array(cost))
    grads <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;dw&#34;</span>: dw,
             <span style=color:#e6db74>&#34;db&#34;</span>: db}
    <span style=color:#66d9ef>return</span> grads, cost


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>optimize</span>(w, b, X, Y, num_iterations<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.009</span>, print_cost<span style=color:#f92672>=</span>False):
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_iterations):
            grads, cost <span style=color:#f92672>=</span> propagate(w, b, X, Y)
            dw <span style=color:#f92672>=</span> grads[<span style=color:#e6db74>&#34;dw&#34;</span>]
            db <span style=color:#f92672>=</span> grads[<span style=color:#e6db74>&#34;db&#34;</span>]
            w <span style=color:#f92672>=</span> w <span style=color:#f92672>-</span> learning_rate<span style=color:#f92672>*</span>dw;
            b <span style=color:#f92672>=</span> b <span style=color:#f92672>-</span> learning_rate<span style=color:#f92672>*</span>db;
</code></pre></div><h3 id=15-python-skill>1.5 Python Skill</h3><p><strong>Broadcasting</strong></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>A <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>],
			[<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>,<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>],
			[<span style=color:#ae81ff>9</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>11</span>,<span style=color:#ae81ff>12</span>])
cal <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># axis=0垂直方向求和,axis=1水平方向求和</span>
percentage<span style=color:#f92672>=</span>A<span style=color:#f92672>/</span>cal<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>4</span>) <span style=color:#75715e># broadcasting(1,4)-&gt;(3,4))</span>
</code></pre></div><blockquote><p>不要用rank=1(秩为1)的array,而要用5*1的矩阵</p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>) <span style=color:#75715e># bad</span>
a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>1</span>) <span style=color:#75715e># good</span>
<span style=color:#66d9ef>assert</span>(a<span style=color:#f92672>.</span>shape<span style=color:#f92672>==</span>(<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>1</span>))
</code></pre></div><h2 id=2-one-hidden-layer-neural-network>2 One hidden layer Neural Network</h2><h3 id=21-neural-network-representation>2.1 Neural Network Representation</h3><blockquote><p>One hidden layer NN also called a two layer NN (不算输入层)</p></blockquote><p><img src=/images/posts/ML/image-20210807193618855.png alt=image-20210807193618855></p><h3 id=22-forward-propagation>2.2 Forward Propagation</h3><h4 id=on-usingle-training-exampleu>On <u>single training example</u>:</h4><p>$$
z^{[1]}=W^{[1]} x+b^{[1]}, a^{[1]}=\sigma(z^{[1]})
$$</p><p>$$
z^{[2]}=W^{[2]} a^{[1]}+b^{[2]}, a^{[2]}=\sigma(z^{[2]})
$$</p><h4 id=vectorizing-across-umultiple-examplesu>vectorizing across <u>multiple examples</u>:</h4><p><img src=/images/posts/ML/image-20210808114345333.png alt=image-20210808114345333>
$$
Z^{[1]} =W^{[1]} X+b^{[1]},A^{[1]}=g^{[1]}(Z^{[1]}),
$$</p><p>$$
Z^{[2]} =W^{[2]} A^{[1]}+b^{[2]},A^{[2]}=g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})
$$</p><h3 id=23-activation-functions>2.3 Activation functions</h3><ul><li>tanh: $a=\frac{e^z-e^{-z}}{e^z+e^{-z}}$</li></ul><blockquote><p>tanh函数在绝大多数场景下比$\sigma$​函数更适合用作激活函数；唯独在binary classification的输出层，$\sigma$​​函数更适合用作激活函数。</p></blockquote><ul><li>Relu: a=max(0, z)</li></ul><blockquote><p>Relu函数可解决梯度消失的问题，优于tanh函数和$\sigma$​函数。除了输出层，默认作为激活函数。</p></blockquote><ul><li>Leaky Relu: a = max(0.01z, z)</li></ul><p><img src=/images/posts/ML/image-20210808114900557.png alt=image-20210808114900557></p><h3 id=24-derivatives-of-activation-functions>2.4 Derivatives of Activation functions</h3><ul><li>g(z)=$\sigma$​(z): $g'(z)=a(1-a)$</li><li>g(z)=tanh(z): $g'(z)=1-a^2$​</li><li>ReLU:</li></ul><p>$$
g'(z)=\left\{
\begin{aligned}
0 & & z&lt;0 \\ 1 & & z\geq0
\end{aligned}
\right.
$$</p><ul><li>Leaky ReLU:</li></ul><p>$$
g'(z)=\left\{
\begin{aligned}
0.01 & & z&lt;0 \\ 1 & & z\geq0
\end{aligned}
\right.
$$</p><h3 id=25-gradient-descent-for-neural-networks>2.5 Gradient descent for neural networks</h3><p>backward propagation:</p><p>$$
dZ^{[2]}=A^{[2]}-Y
$$</p><p>$$
dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T}
$$</p><p>$$
db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdim=True)
$$</p><p>$$
d Z^{[1]}=W^{[2] T} d Z^{[2]} * g^{[1]^{\prime}}\left(\mathrm{Z}^{[1]}\right)
$$</p><p>$$
d W^{[1]}=\frac{1}{m} d Z^{[1]} X^{T}
$$</p><p>$$
d b^{[1]}=\frac{1}{m} np.sum(d Z^{[1]}, axis=1, keepdims=True)
$$</p><ul><li>keepdim=True: 用于防止输出$(n^{[2]},)$，而是输出$(n^{[2]},1)$</li></ul><h3 id=26-random-initialization>2.6 Random Initialization</h3><blockquote><p>在Neural network中，w不能初始化为全0。</p></blockquote><p>$$
w^{[1]}=np.random.randn((2,2))*0.01
$$</p><p>$$
b^{[2]}=np.zero((2,1))
$$</p><p>$$
w^{[1]}=np.random.randn((1,2))*0.01
$$</p><p>$$
b^{[2]}=0
$$</p><h2 id=3-deep-neural-network>3 Deep Neural Network</h2><ul><li>layers: L = 4</li><li>ouput: $\hat{y}=a^{[L]}$</li></ul><h3 id=31-forward-propagation>3.1 Forward Propagation</h3><p>for l=1&mldr;4:
$$
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}
$$</p><p>$$
A^{[Ll}=g^{[l]}(Z^{[l]})
$$</p><h3 id=32-backward-propagation>3.2 Backward Propagation</h3><ul><li>Input $da^{[l]}$​</li><li>Output $da^{[l-1},dW^{[l]},db^{[l]}$</li></ul><p>$$
dZ^{[l]}=dA^{[l]}*{g^{[l]}}'(Z^{[l]})
$$</p><p>$$
dW^{[l]}=\frac{1}{m}dZ^{[l]}·A^{[l-1]T}
$$</p><p>$$
db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},AXIS=1, keepdims=True)
$$</p><p>$$
dA^{[l-1]}=W^{[l]T}dZ^{[l]}
$$</p><h3 id=33-matrix-dimensions>3.3 Matrix dimensions</h3><ul><li>$W^{[l]},dW^{[l]}:(n^{[l]},n^{[l-1]})$</li><li>$b^{[l]},db^{[l]}:(n^{[l]},1)$</li><li>$z^{[l]},a^{[l]}:(n^{[l]},1)$</li><li>$Z^{[l]},A^{[l]},dZ^{[l]},dA^{[l]}:(n^{[l]},m)$</li></ul><h3 id=34-working-procedure>3.4 Working procedure</h3><p><img src=/images/posts/ML/image-20210810111918024.png alt=image-20210810111918024></p><h3 id=35-parameters--hyperparameters>3.5 Parameters & Hyperparameters</h3><blockquote><p>Hyperparameters control the parameters.</p></blockquote><p>Paramerters:</p><ul><li>$W^{[a]},b^{[a]}$​</li></ul><p>Hyperparematers:</p><ul><li>learning rate $\alpha$​</li><li>iterations</li><li>hidden layers L</li><li>&mldr;&mldr;</li></ul></div><div class=btn-improve-page><a href=https://github.com/EricWang007/ericwang007.github.io/edit/main/content/posts/ML/DL%e5%9f%ba%e7%a1%80.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/ml/cv%E5%9F%BA%E7%A1%80/ title=CV基础 class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Prev</div><div class=next-prev-text>CV基础</div></a></div><div class="col-md-6 next-article"><a href=/posts/ml/dl%E5%BA%94%E7%94%A8/ title=DL应用 class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>DL应用</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#1-logistic-regression-model>1 Logistic Regression Model</a><ul><li><a href=#11-binary-classification>1.1 Binary Classification</a></li><li><a href=#12-logistic-regression>1.2 Logistic Regression</a></li><li><a href=#13-cost-function>1.3 Cost Function</a></li><li><a href=#14-gradient-descent>1.4 Gradient Descent</a></li><li><a href=#15-python-skill>1.5 Python Skill</a></li></ul></li><li><a href=#2-one-hidden-layer-neural-network>2 One hidden layer Neural Network</a><ul><li><a href=#21-neural-network-representation>2.1 Neural Network Representation</a></li><li><a href=#22-forward-propagation>2.2 Forward Propagation</a></li><li><a href=#23-activation-functions>2.3 Activation functions</a></li><li><a href=#24-derivatives-of-activation-functions>2.4 Derivatives of Activation functions</a></li><li><a href=#25-gradient-descent-for-neural-networks>2.5 Gradient descent for neural networks</a></li><li><a href=#26-random-initialization>2.6 Random Initialization</a></li></ul></li><li><a href=#3-deep-neural-network>3 Deep Neural Network</a><ul><li><a href=#31-forward-propagation>3.1 Forward Propagation</a></li><li><a href=#32-backward-propagation>3.2 Backward Propagation</a></li><li><a href=#33-matrix-dimensions>3.3 Matrix dimensions</a></li><li><a href=#34-working-procedure>3.4 Working procedure</a></li><li><a href=#35-parameters--hyperparameters>3.5 Parameters & Hyperparameters</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>2018211947@bupt.edu.cn</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">By entering your email address, you agree to receive the newsletter of this website.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><!doctype html></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},],strict:false,throwOnError:false});});</script></body></html>