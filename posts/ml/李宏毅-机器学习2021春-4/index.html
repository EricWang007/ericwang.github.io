<!doctype html><html><head><title>李宏毅-机器学习2021春-4</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><meta property="og:title" content="李宏毅-机器学习2021春-4"><meta property="og:description" content="李宏毅-机器学习2021春-4  1 Transformer Sequence-to-sequence（Seq2seq）
 输出的长度由模型决定 Encoder+Decoder  1.1 Encoder encoder内部由许多block组成：
每个block的构成如下：
1.2 Decoder-Autoregressive(AT) 与Encoder的对比图：（Multi-Head Attention前加了一个Masked）
Masked Self-attention  产生$b^1$的时候只能考虑$b^1$的资讯。 产生$b^2$的时候只能考虑$b^1$、$b^2$的资讯。 产生$b^3$的时候只能考虑$b^1$、$b^2$、$b^3$的资讯。  1.3 Encoder-Decoder Cross attention:
Teacher Forcing: 用ground truth（答案）作为Decoder的输入
1.4 Training Tips  Copy Mechanism Guided Attention Beam Search：不基于贪心的一种搜索算法  2 BERT  Self-supervised Learning  系统通过输入数据的一部分进行predict，另一部分输入用于进行比对。
 Masking Input   Next Sentence Prediction  对于BERT不是很有用     BERT经过Fine-tune，可用于下游任务。
  BERT整体是Semi-supervised的。填空题（Pre-train）阶段是Self-supervised，Finetune阶段是supervised。
 2.1 Bert Case  用于语义分析。输出类别。BERT是用填空题预训练的。   用于词性标注。输出和输出长度相同。   用于自然语言推测。输入两个句子，输出一个类别。   基于提取的问答系统  输入一个问题和一篇文档，输出答案的起始位置和结束位置。    2."><meta property="og:type" content="article"><meta property="og:url" content="https://ericwang007.github.io/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-4/"><meta property="article:published_time" content="2021-10-17T06:00:20+06:00"><meta property="article:modified_time" content="2021-10-17T06:00:20+06:00"><meta name=description content="李宏毅-机器学习2021春-4"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>EricWang's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithmdatastructure/>Algorithm&DataStructure</a><ul><li><a href=/posts/algorithmdatastructure/%E6%8E%92%E5%BA%8F%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/ title=排序方法总结>排序方法总结</a></li><li><a href=/posts/algorithmdatastructure/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/ title=最小生成树>最小生成树</a></li><li><a href=/posts/algorithmdatastructure/%E7%BA%A2%E9%BB%91%E6%A0%91/ title=红黑树>红黑树</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/bigdata/>BigData</a><ul><li><a href=/posts/bigdata/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6/ title=大数据组件>大数据组件</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database/>Database</a><ul><li><a href=/posts/database/1-introduction/ title="1 Introduction">1 Introduction</a></li><li><a href=/posts/database/2-relation-database/ title="Relation Database">Relation Database</a></li><li><a href=/posts/database/3-sql/ title=SQL>SQL</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/java/>Java</a><ul><li><a href=/posts/java/java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/ title=Java基础语法>Java基础语法</a></li><li><a href=/posts/java/java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/ title=Java集合框架>Java集合框架</a></li><li><a href=/posts/java/java%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/ title=Java面试问题>Java面试问题</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/linux/>Linux</a><ul><li><a href=/posts/linux/part-1/ title="Part 1">Part 1</a></li><li><a href=/posts/linux/part-2/ title="Part 2">Part 2</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/ml/>ML</a><ul class=active><li><a href=/posts/ml/andrewng-cv%E5%9F%BA%E7%A1%80/ title=AndrewNG-CV基础>AndrewNG-CV基础</a></li><li><a href=/posts/ml/andrewng-dl%E5%9F%BA%E7%A1%80/ title=AndrewNG-DL基础>AndrewNG-DL基础</a></li><li><a href=/posts/ml/andrewng-dl%E5%BA%94%E7%94%A8/ title=AndrewNG-DL应用>AndrewNG-DL应用</a></li><li><a href=/posts/ml/andrewng-gan%E5%9F%BA%E7%A1%80/ title=AndrewNG-GAN基础>AndrewNG-GAN基础</a></li><li><a href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-1/ title=李宏毅-机器学习2021春-1>李宏毅-机器学习2021春-1</a></li><li><a href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-2/ title=李宏毅-机器学习2021春-2>李宏毅-机器学习2021春-2</a></li><li><a href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-3/ title=李宏毅-机器学习2021春-3>李宏毅-机器学习2021春-3</a></li><li><a class=active href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-4/ title=李宏毅-机器学习2021春-4>李宏毅-机器学习2021春-4</a></li><li><a href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-5/ title=李宏毅-机器学习2021春-5>李宏毅-机器学习2021春-5</a></li><li><a href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-6/ title=李宏毅-机器学习2021春-6>李宏毅-机器学习2021春-6</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/os/>OS</a><ul><li><a href=/posts/os/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86-1/ title="知识梳理 1">知识梳理 1</a></li><li><a href=/posts/os/%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86-2/ title="知识梳理 2">知识梳理 2</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/sdn/>SDN</a><ul><li><a href=/posts/sdn/sdn-1/ title="SDN 1">SDN 1</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://ericwang007.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/%E5%86%88%E5%B4%8E%E6%9C%8B%E4%B9%9F_huadb4a3dbe9f5476167756f348ab52644_35855_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Eric Wang</h5><p>October 17, 2021</p></div><div class=title><h1>李宏毅-机器学习2021春-4</h1></div><div class=post-content id=post-content><h1 id=李宏毅-机器学习2021春-4>李宏毅-机器学习2021春-4</h1><hr><h2 id=1-transformer>1 Transformer</h2><p>Sequence-to-sequence（Seq2seq）</p><ul><li>输出的长度由模型决定</li><li>Encoder+Decoder</li></ul><h3 id=11-encoder>1.1 Encoder</h3><p><img src=/images/posts/ML/image-20211019225359320.png alt=image-20211019225359320 style=zoom:67%></p><p>encoder内部由许多block组成：</p><p><img src=/images/posts/ML/image-20211020222703746.png alt=image-20211020222703746 style=zoom:67%></p><p>每个block的构成如下：</p><p><img src=/images/posts/ML/image-20211019225247756.png alt=image-20211019225247756 style=zoom:67%></p><h3 id=12-decoder-autoregressiveat>1.2 Decoder-Autoregressive(AT)</h3><p><img src=/images/posts/ML/image-20211020225509446.png alt=image-20211020225509446 style=zoom:67%></p><p><img src=/images/posts/ML/image-20211020225552527.png alt=image-20211020225552527 style=zoom:50%></p><p>与Encoder的对比图：（Multi-Head Attention前加了一个Masked）</p><p><img src=/images/posts/ML/image-20211020224951515.png alt=image-20211020224951515 style=zoom:67%></p><h4 id=masked-self-attention>Masked Self-attention</h4><ul><li>产生$b^1$的时候只能考虑$b^1$的资讯。</li><li>产生$b^2$的时候只能考虑$b^1$、$b^2$的资讯。</li><li>产生$b^3$的时候只能考虑$b^1$、$b^2$、$b^3$的资讯。</li></ul><p><img src=/images/posts/ML/image-20211020225141425.png alt=image-20211020225141425 style=zoom:67%></p><h3 id=13-encoder-decoder>1.3 Encoder-Decoder</h3><p><img src=/images/posts/ML/image-20211020225741366.png alt=image-20211020225741366 style=zoom:67%></p><p>Cross attention:</p><p><img src=/images/posts/ML/image-20211020225944344.png alt=image-20211020225944344 style=zoom:67%></p><p>Teacher Forcing: 用ground truth（答案）作为Decoder的输入</p><h3 id=14-training-tips>1.4 Training Tips</h3><ul><li>Copy Mechanism</li><li>Guided Attention</li><li>Beam Search：不基于贪心的一种搜索算法</li></ul><h2 id=2-bert>2 BERT</h2><ul><li>Self-supervised Learning</li></ul><p>系统通过输入数据的一部分进行predict，另一部分输入用于进行比对。</p><p><img src=/images/posts/ML/image-20211021214159750.png alt=image-20211021214159750 style=zoom:67%></p><ul><li>Masking Input</li></ul><p><img src=/images/posts/ML/image-20211021214819411.png alt=image-20211021214819411 style=zoom:67%></p><ul><li>Next Sentence Prediction<ul><li>对于BERT不是很有用</li></ul></li></ul><p><img src=/images/posts/ML/image-20211022214702559.png alt=image-20211022214702559 style=zoom:67%></p><blockquote><p>BERT经过Fine-tune，可用于下游任务。</p></blockquote><blockquote><p>BERT整体是Semi-supervised的。填空题（Pre-train）阶段是Self-supervised，Finetune阶段是supervised。</p></blockquote><h3 id=21-bert-case>2.1 Bert Case</h3><ul><li>用于语义分析。输出类别。BERT是用填空题预训练的。</li></ul><p><img src=/images/posts/ML/image-20211022221836853.png alt=image-20211022221836853 style=zoom:67%></p><ul><li>用于词性标注。输出和输出长度相同。</li></ul><p><img src=/images/posts/ML/image-20211023143128238.png alt=image-20211023143128238 style=zoom:67%></p><ul><li>用于自然语言推测。输入两个句子，输出一个类别。</li></ul><p><img src=/images/posts/ML/image-20211023145817021.png alt=image-20211023145817021 style=zoom:67%></p><p><img src=/images/posts/ML/image-20211023145852772.png alt=image-20211023145852772 style=zoom:67%></p><ul><li>基于提取的问答系统<ul><li>输入一个问题和一篇文档，输出答案的起始位置和结束位置。</li></ul></li></ul><p><img src=/images/posts/ML/image-20211023150225152.png alt=image-20211023150225152 style=zoom:67%></p><p><img src=/images/posts/ML/image-20211024121230026.png alt=image-20211024121230026 style=zoom:67%></p><h3 id=22-bert的其它应用>2.2 Bert的其它应用</h3><p>Bert可以应用于蛋白质分类、DNA分类、音乐分类。</p><h4 id=multi-lingual-bert>Multi-lingual BERT</h4><p>用多种语言进行填空题训练。</p><h2 id=3-gpt>3 GPT</h2></div><div class=btn-improve-page><a href=https://github.com/EricWang007/ericwang007.github.io/edit/main/content/posts/ML/%e6%9d%8e%e5%ae%8f%e6%af%85-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a02021%e6%98%a5-4.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-3/ title=李宏毅-机器学习2021春-3 class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Prev</div><div class=next-prev-text>李宏毅-机器学习2021春-3</div></a></div><div class="col-md-6 next-article"><a href=/posts/ml/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021%E6%98%A5-5/ title=李宏毅-机器学习2021春-5 class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>李宏毅-机器学习2021春-5</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#1-transformer>1 Transformer</a><ul><li><a href=#11-encoder>1.1 Encoder</a></li><li><a href=#12-decoder-autoregressiveat>1.2 Decoder-Autoregressive(AT)</a></li><li><a href=#13-encoder-decoder>1.3 Encoder-Decoder</a></li><li><a href=#14-training-tips>1.4 Training Tips</a></li></ul></li><li><a href=#2-bert>2 BERT</a><ul><li><a href=#21-bert-case>2.1 Bert Case</a></li><li><a href=#22-bert的其它应用>2.2 Bert的其它应用</a></li></ul></li><li><a href=#3-gpt>3 GPT</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>2018211947@bupt.edu.cn</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">By entering your email address, you agree to receive the newsletter of this website.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><!doctype html></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body);></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},],strict:false,throwOnError:false});});</script></body></html>