---

title: "李宏毅-机器学习2021春-1"
date: 2021-10-15T06:00:20+06:00
hero: /images/posts/writing-posts/hugo-logo.svg
math: true
menu:
  sidebar:
    name: 李宏毅-机器学习2021春-1
    identifier: 李宏毅-机器学习2021春-1
    parent: ML
    weight: 10
---

# 李宏毅-机器学习2021春-1

---

## 1 机器学习基本概念

### 1.1 机器学习基本任务

<u>机器学习的基本任务：**寻找一个函数**</u>

![image-20211015162826661](/images/posts/ML/image-20211015162826661.png)

不同种类的函数：

* **Regression（回归）**：函数输出一个标量
  * 如：预测PM2.5
* **Clssificatiion（分类）**：给定选项，函数输出选项
  * 如：Alpha Go下棋
* **Structured Learning**：创造一些结构（图片，文件）

### 1.2 通过训练数据定义Loss

Loss 也是一个函数，它的输入是Model中的parameters：
$$
L(b,w)
$$

Loss function：$L=\frac{1}{N}\sum_ne_n$

* Mean Absolute Error(MAE)：$e=|y-\hat{y}|$
* Mean Square Error(MSE)：$e=(y-\hat{y})^2$

![image-20211015164043176](/images/posts/ML/image-20211015164043176.png)

### 1.3 Optimization

目标：得到最优的参数。
$$
w^{*}, b^{*}=\arg \min _{w, b} L
$$
方式：**Gradient Descent**

* 一个参数w的情况
  * 随机选取初始值$w^0$
  * 计算$\left.\frac{\partial L}{\partial \mathcal{\imath}}\right|_{w=w^{0}}$
  * learning rate：$\eta$，表示梯度下降的速率
  * 不断更新w：$w^{1} \leftarrow w^{0}-\left.\eta \frac{\partial L}{\partial w}\right|_{w=w^{0}}$

![image-20211015164644911](/images/posts/ML/image-20211015164644911.png)

* 两个参数的情况：

<img src="/images/posts/ML/image-20211015183919623.png" alt="image-20211015183919623" style="zoom:60%;" />

![image-20211015165202445](/images/posts/ML/image-20211015165202445.png)

## 2 深度学习基本概念

### 2.1 Function

使用Piecewise Linear Curves逼近连续的曲线。

<img src="/images/posts/ML/image-20211015185500819.png" alt="image-20211015185500819" style="zoom:50%;" />

Piecewise Linear Curves = constant  + Sigmoid函数

 <img src="/images/posts/ML/image-20211015191315921.png" alt="image-20211015191315921" style="zoom:67%;" />

调整w、b、c，得到不同的Sigmoid Function

<img src="/images/posts/ML/image-20211015191419143.png" alt="image-20211015191419143" style="zoom:67%;" /> 

以此构造神经网络。

<img src="/images/posts/ML/image-20211016113014591.png" alt="image-20211016113014591" style="zoom:80%;" />

<img src="/images/posts/ML/image-20211016113125615.png" alt="image-20211016113125615" style="zoom:70%;" /> 

### 2.2 Loss

Loss：与之前相同，输入同样是一堆参数。

![image-20211016212625286](/images/posts/ML/image-20211016212625286.png)

### 2.3 Optimization

$$
\theta^*=arg min_\theta L
$$

![image-20211016213634490](/images/posts/ML/image-20211016213634490.png)

![image-20211016213751837](/images/posts/ML/image-20211016213751837.png)

* 每次更新一次参数叫做一次update，吧所有的batch都看过以便叫做一个epoch。

#### 反向传播

从后往前计算偏导数，得到完整的梯度。

![image-20211017201759834](/images/posts/ML/image-20211017201759834.png)

### 2.3 Sigmoid->ReLU

两个ReLu叠起来构成一个Hard Sigmoid。

![image-20211016214042044](/images/posts/ML/image-20211016214042044.png)

