---

title: "李宏毅-机器学习2021春-4"
date: 2021-10-17T06:00:20+06:00
hero: /images/posts/writing-posts/hugo-logo.svg
math: true
menu:
  sidebar:
    name: 李宏毅-机器学习2021春-4
    identifier: 李宏毅-机器学习2021春-4
    parent: ML
    weight: 10
---

# 李宏毅-机器学习2021春-4

---

## 1 Transformer

Sequence-to-sequence（Seq2seq）

* 输出的长度由模型决定
* Encoder+Decoder

### 1.1 Encoder

<img src="/images/posts/ML/image-20211019225359320.png" alt="image-20211019225359320" style="zoom:67%;" /> 

encoder内部由许多block组成：

<img src="/images/posts/ML/image-20211020222703746.png" alt="image-20211020222703746" style="zoom:67%;" /> 

每个block的构成如下：

<img src="/images/posts/ML/image-20211019225247756.png" alt="image-20211019225247756" style="zoom:67%;" /> 

### 1.2 Decoder-Autoregressive(AT)

<img src="/images/posts/ML/image-20211020225509446.png" alt="image-20211020225509446" style="zoom:67%;" />  

<img src="/images/posts/ML/image-20211020225552527.png" alt="image-20211020225552527" style="zoom:50%;" /> 

与Encoder的对比图：（Multi-Head Attention前加了一个Masked）

<img src="/images/posts/ML/image-20211020224951515.png" alt="image-20211020224951515" style="zoom:67%;" /> 

#### Masked Self-attention

* 产生$b^1$的时候只能考虑$b^1$的资讯。
* 产生$b^2$的时候只能考虑$b^1$、$b^2$的资讯。
* 产生$b^3$的时候只能考虑$b^1$、$b^2$、$b^3$的资讯。

<img src="/images/posts/ML/image-20211020225141425.png" alt="image-20211020225141425" style="zoom:67%;" /> 

### 1.3 Encoder-Decoder

<img src="/images/posts/ML/image-20211020225741366.png" alt="image-20211020225741366" style="zoom:67%;" /> 

Cross attention:

<img src="/images/posts/ML/image-20211020225944344.png" alt="image-20211020225944344" style="zoom:67%;" /> 

Teacher Forcing: 用ground truth（答案）作为Decoder的输入

### 1.4 Training Tips

* Copy Mechanism
* Guided Attention
* Beam Search：不基于贪心的一种搜索算法

## 2 BERT

* Self-supervised Learning

系统通过输入数据的一部分进行predict，另一部分输入用于进行比对。

<img src="/images/posts/ML/image-20211021214159750.png" alt="image-20211021214159750" style="zoom:67%;" /> 

* Masking Input 

<img src="/images/posts/ML/image-20211021214819411.png" alt="image-20211021214819411" style="zoom:67%;" /> 

* Next Sentence Prediction
  * 对于BERT不是很有用

<img src="/images/posts/ML/image-20211022214702559.png" alt="image-20211022214702559" style="zoom:67%;" /> 

>  BERT经过Fine-tune，可用于下游任务。

>  BERT整体是Semi-supervised的。填空题（Pre-train）阶段是Self-supervised，Finetune阶段是supervised。

### 2.1 Bert Case

* 用于语义分析。输出类别。BERT是用填空题预训练的。

<img src="/images/posts/ML/image-20211022221836853.png" alt="image-20211022221836853" style="zoom:67%;" /> 

* 用于词性标注。输出和输出长度相同。

<img src="/images/posts/ML/image-20211023143128238.png" alt="image-20211023143128238" style="zoom:67%;" /> 

* 用于自然语言推测。输入两个句子，输出一个类别。

<img src="/images/posts/ML/image-20211023145817021.png" alt="image-20211023145817021" style="zoom:67%;" /> 

<img src="/images/posts/ML/image-20211023145852772.png" alt="image-20211023145852772" style="zoom:67%;" /> 

* 基于提取的问答系统
  * 输入一个问题和一篇文档，输出答案的起始位置和结束位置。

<img src="/images/posts/ML/image-20211023150225152.png" alt="image-20211023150225152" style="zoom:67%;" /> 

<img src="/images/posts/ML/image-20211024121230026.png" alt="image-20211024121230026" style="zoom:67%;" /> 

### 2.2 Bert的其它应用

Bert可以应用于蛋白质分类、DNA分类、音乐分类。

#### Multi-lingual BERT

用多种语言进行填空题训练。 

## 3 GPT















