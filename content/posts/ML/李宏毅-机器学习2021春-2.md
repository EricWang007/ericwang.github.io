---

title: "李宏毅-机器学习2021春-2"
date: 2021-10-17T06:00:20+06:00
hero: /images/posts/writing-posts/hugo-logo.svg
math: true
menu:
  sidebar:
    name: 李宏毅-机器学习2021春-2
    identifier: 李宏毅-机器学习2021春-2
    parent: ML
    weight: 10
---

# 李宏毅-机器学习2021春-2

---

## 1 机器学习任务攻略

![image-20211017200943668](/images/posts/ML/image-20211017200943668.png)

* Model bias：模型过于简单，有局限性

* Optimization Issue：模型足够复杂了，但Optimization做的不够好。

  <img src="/images/posts/ML/image-20211017201139668.png" alt="image-20211017201139668" style="zoom:67%;" />

* Overfitting：在训练集上效果好，在测试集上效果差。解决方法如下：

  1. 增加训练数据，Data augmentation（图片左右翻转）
  2. 限制模型，减少模型的flexibility：
     * 减少参数，共享参数（CNN）
     * 减少features
     * Early stopping
     * Regularization
     * Dropout

* 模型的复杂性需要在Overfitting和Model bias间进行trade-off

  <img src="/images/posts/ML/image-20211017202556724.png" alt="image-20211017202556724" style="zoom:67%;" />

* mismatch：训练集和测试集的数据分别不同

  <img src="/images/posts/ML/image-20211017202958149.png" alt="image-20211017202958149" style="zoom:80%;" /> 

* N-fold Cross Validation：将Training Set进行不同的分组，分别对模型进行训练和验证，得到的mse取平均值。

  <img src="/images/posts/ML/image-20211017202848671.png" alt="image-20211017202848671" style="zoom:80%;" /> 

## 2 类神经网络训练不起来怎么办

### 2.1 Local Minima与Saddle Point

Critical point包括Local Minima与Saddle Point。

当有很多参数时，Local Minima几乎不存在。 

### 2.2 Batch与Momentum

#### Batch

<img src="/images/posts/ML/image-20211017214252193.png" alt="image-20211017214252193" style="zoom:67%;" /> 

* Shuffle：在一个epoch重新分batch。

大Batch v.s. 小Batch

* 当考虑并行运算（gpu）后，大Batch的运行速度更快。

* 小Batch的noise更大，但这反而有助于训练。

  <img src="/images/posts/ML/image-20211017221145504.png" alt="image-20211017221145504" style="zoom:67%;" /> 

结论：小Batch效果更好，但耗时间更多。

<img src="/images/posts/ML/image-20211017221723883.png" alt="image-20211017221723883" style="zoom:67%;" /> 

#### Momentum

<img src="/images/posts/ML/image-20211017222007662.png" alt="image-20211017222007662" style="zoom:67%;" /> 

<img src="/images/posts/ML/image-20211017222243933.png" alt="image-20211017222243933" style="zoom:67%;" /> 

### 2.3 Adaptive Learning Rate

$$
\theta_i^{t+1}\leftarrow\theta_i^t-\frac{\eta}{\sigma^t_i}g^t_i
$$

* Root Mean Square:

  * <img src="/images/posts/ML/image-20211018152743652.png" alt="image-20211018152743652" style="zoom:60%;" /> 
  * Used in Adagrad

* RMSProp:

  * <img src="/images/posts/ML/image-20211018153513599.png" alt="image-20211018153513599" style="zoom:60%;" />

* Learning Rate Scheduling

  <img src="/images/posts/ML/image-20211018153915627.png" alt="image-20211018153915627" style="zoom:60%;" />

  * Learning Rate Decay：<img src="/images/posts/ML/image-20211018154205233.png" alt="image-20211018154205233" style="zoom:60%;" />
  * Warm Up：<img src="/images/posts/ML/image-20211018154229848.png" alt="image-20211018154229848" style="zoom:60%;" />
  

#### Summay of Optimization

<img src="/images/posts/ML/image-20211018155048545.png" alt="image-20211018155048545" style="zoom:67%;" /> 

### 2.4 Batch Normalization

####   Feature Normalization

<img src="/images/posts/ML/image-20211018164201732.png" alt="image-20211018164201732" style="zoom:67%;" /> 

在实际中，由于$z^1$的改变，会引起$\mu和\sigma$的改变，进而引起$\widetilde z^1、\widetilde z^2、\widetilde z^3$的改变，因此，整个Batch组成了一个大的神经网络。

<img src="/images/posts/ML/image-20211018165405706.png" alt="image-20211018165405706" style="zoom:67%;" />  

#### Batch Normalization

$\gamma$和$\beta$一开始是全1和全0，它们作为参数，被机器学习。 

<img src="/images/posts/ML/image-20211018170308367.png" alt="image-20211018170308367" style="zoom:67%;" /> 









