---

title: "李宏毅-机器学习2021春-3"
date: 2021-10-17T06:00:20+06:00
hero: /images/posts/writing-posts/hugo-logo.svg
math: true
menu:
  sidebar:
    name: 李宏毅-机器学习2021春-3
    identifier: 李宏毅-机器学习2021春-3
    parent: ML
    weight: 10
---

# 李宏毅-机器学习2021春-3

---

## 1 Classification

将Class用`one-hot vector`表示

<img src="../../../static/images/posts/ML/image-20211018171800758.png" alt="image-20211018171800758" style="zoom:80%;" /> 

回归与分类的区别：

<img src="../../../static/images/posts/ML/image-20211018172028857.png" alt="image-20211018172028857" style="zoom:67%;" /> 

softmax：

<img src="../../../static/images/posts/ML/image-20211018204256372.png" alt="image-20211018204256372" style="zoom:67%;" /> 

Loss的计算：

<img src="../../../static/images/posts/ML/image-20211018212606688.png" alt="image-20211018212606688" style="zoom:67%;" /> 

## 2 Convolutional Neural Network (CNN)





## 3 Self-attention

### 3.1 背景

当输入是多组向量时，输出的情况：

* 每一个向量都有一个label（sequence labeling），用到了**self-attention**
* 整个sequence有一个label
* 模型自己决定输出长度。

<img src="../../../static/images/posts/ML/image-20211019220621798.png" alt="image-20211019220621798" style="zoom:50%;" /> 

### 3.2 原理

Self-attention：根据a向量之间的关联性，相应的b向量

<img src="../../../static/images/posts/ML/image-20211019222613111.png" alt="image-20211019222613111" style="zoom:50%;" /> 

解释得到$b^1$的过程：

* <img src="../../../static/images/posts/ML/image-20211019221902427.png" alt="image-20211019221902427" style="zoom:50%;" /> 
*  ●代表dot-product；此处不一定要用soft-max
* <img src="../../../static/images/posts/ML/image-20211019223447121.png" alt="image-20211019223447121" style="zoom:50%;" /> 

完整过程：

* <img src="../../../static/images/posts/ML/image-20211019223658109.png" alt="image-20211019223658109" style="zoom:50%;" /> 
* <img src="../../../static/images/posts/ML/image-20211019223737019.png" alt="image-20211019223737019" style="zoom:50%;" /> 

### 3.3 变种

在语音辨识中，用于输入的数据很长，可以用Trancated Self-attention，只考虑每一段音频和周围的一定长度内的音频的关系。











