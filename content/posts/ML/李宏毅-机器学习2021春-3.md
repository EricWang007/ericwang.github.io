---

title: "李宏毅-机器学习2021春-3"
date: 2021-10-17T06:00:20+06:00
hero: /images/posts/writing-posts/hugo-logo.svg
math: true
menu:
  sidebar:
    name: 李宏毅-机器学习2021春-3
    identifier: 李宏毅-机器学习2021春-3
    parent: ML
    weight: 10
---

# 李宏毅-机器学习2021春-3

---

## 1 Classification

将Class用`one-hot vector`表示

<img src="/images/posts/ML/image-20211018171800758.png" alt="image-20211018171800758" style="zoom:80%;" /> 

回归与分类的区别：

<img src="/images/posts/ML/image-20211018172028857.png" alt="image-20211018172028857" style="zoom:67%;" /> 

softmax：

<img src="/images/posts/ML/image-20211018204256372.png" alt="image-20211018204256372" style="zoom:67%;" /> 

Loss的计算：

<img src="/images/posts/ML/image-20211018212606688.png" alt="image-20211018212606688" style="zoom:67%;" /> 

## 2 Convolutional Neural Network (CNN)





## 3 Self-attention

### 3.1 背景

当输入是多组向量时，输出的情况：

* 每一个向量都有一个label（sequence labeling），<u>此处可以用**self-attention**</u>
* 整个sequence有一个label
* 模型自己决定输出长度（seq to seq）

<img src="/images/posts/ML/image-20211019220621798.png" alt="image-20211019220621798" style="zoom:50%;" /> 

### 3.2 原理

Self-attention的输出$b^1$，既代表$a^1$，又代表$a^1$和$a^2$、$a^3$的关系。

<img src="/images/posts/ML/image-20211019222613111.png" alt="image-20211019222613111" style="zoom:50%;" /> 

解释得到$b^1$的过程：

* <img src="/images/posts/ML/image-20211019221902427.png" alt="image-20211019221902427" style="zoom:50%;" /> 
*  ●代表dot-product；此处不一定要用soft-max
* <img src="/images/posts/ML/image-20211019223447121.png" alt="image-20211019223447121" style="zoom:50%;" /> 

完整过程：

* <img src="/images/posts/ML/image-20211019223658109.png" alt="image-20211019223658109" style="zoom:50%;" /> 
* <img src="/images/posts/ML/image-20211019223737019.png" alt="image-20211019223737019" style="zoom:50%;" /> 

### 3.3 Multi-head Self-attention

寻找多种相关性。

<img src="/images/posts/ML/image-20211020223303153.png" alt="image-20211020223303153" style="zoom:67%;" /> 

<img src="/images/posts/ML/image-20211020223323098.png" alt="image-20211020223323098" style="zoom:67%;" /> 

<img src="/images/posts/ML/image-20211020223339129.png" alt="image-20211020223339129" style="zoom:67%;" /> 

#### Positional Encoding

为每一个位置设置一个位置向量$e^i$。

<img src="/images/posts/ML/image-20211020223531893.png" alt="image-20211020223531893" style="zoom:50%;" /> 

### 3.4 其它变种

在语音辨识中，用于输入的数据很长，可以用`Trancated Self-attention`，只考虑每一段音频和周围的一定长度内的音频的关系。

Self-attention同样可以用于图片。

。。。

#### Self-attention vs CNN

。。。

#### Self-attention vs RNN

。。。

#### Self-attention for Graph

。。。







